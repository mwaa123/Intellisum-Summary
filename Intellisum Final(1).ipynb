{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69d52892",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import bs4 as bs \n",
    "import urllib.request \n",
    "import re \n",
    "import nltk\n",
    "import heapq\n",
    "import PyPDF2 \n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from summarizer import Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f471448f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get text from a URL\n",
    "def get_text_from_link(link):\n",
    "    scraped_data = urllib.request.urlopen(link)\n",
    "    article = scraped_data.read()\n",
    "    parsed_article = bs.BeautifulSoup(article, 'lxml')\n",
    "    paragraphs = parsed_article.find_all('p')\n",
    "    article_text = \"\"\n",
    "    for p in paragraphs:\n",
    "        article_text += p.text\n",
    "    return article_text\n",
    "\n",
    "# Function to read text from a PDF\n",
    "def read_pdf(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        text = ''\n",
    "        for page_num in range(len(pdf_reader.pages)):\n",
    "            text += pdf_reader.pages[page_num].extract_text()\n",
    "    return text\n",
    "\n",
    "# Function for text preprocessing (replace this with your implementation)\n",
    "def preprocess_text(text):\n",
    "    # Your text preprocessing logic here\n",
    "    return text\n",
    "\n",
    "# Function for generating a summary (replace this with your implementation)\n",
    "def generate_summary(text):\n",
    "    # Your summarization logic here\n",
    "    return \"Sample summary: This is a placeholder summary.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fa2879d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to summarize text\n",
    "def generate_summary(text, num_sentences=5):\n",
    "    model = Summarizer()\n",
    "    summary = model(text, min_length=60, max_length=num_sentences*60)\n",
    "    return ''.join(summary)\n",
    "\n",
    "\n",
    "    # Word Frequencies\n",
    "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "    word_frequencies = {}\n",
    "    for word in nltk.word_tokenize(text):\n",
    "        if word.lower() not in stop_words:\n",
    "            if word.lower() not in word_frequencies.keys():\n",
    "                word_frequencies[word.lower()] = 1\n",
    "            else:\n",
    "                word_frequencies[word.lower()] += 1\n",
    "\n",
    "    # Normalize word frequencies\n",
    "    max_frequency = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = word_frequencies[word] / max_frequency\n",
    "\n",
    "    # Sentence Scores\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentence_list:\n",
    "        for word, freq in word_frequencies.items():\n",
    "            if word in sentence.lower():\n",
    "                if len(sentence.split(' ')) < 30:\n",
    "                    if sentence not in sentence_scores.keys():\n",
    "                        sentence_scores[sentence] = freq\n",
    "                    else:\n",
    "                        sentence_scores[sentence] += freq\n",
    "\n",
    "    # Get the Summary\n",
    "    summary_sentences = heapq.nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
    "    summary = ' '.join(summary_sentences)\n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e375cf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# URL link\n",
    "url_link = 'https://en.wikipedia.org/wiki/Artificial_intelligence'\n",
    "url_text = get_text_from_link(url_link)\n",
    "\n",
    "# Google link\n",
    "google_link = 'https://anzavillage.africa/'\n",
    "google_text = get_text_from_link(google_link)\n",
    "\n",
    "# PDF path\n",
    "pdf_path = 'C:/Users/admin/Documents/PRINT DUKA PROPOSAL .pdf'\n",
    "pdf_text = read_pdf(pdf_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8a0a65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n",
      "C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\admin\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=1.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary from URL:\n",
      " Artificial intelligence (AI) is the intelligence of machines or software, as opposed to the intelligence of humans or other animals. It is a field of study in computer science that develops and studies intelligent machines. Classifiers[97]\n",
      "are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. It is trained to recognise patterns, once trained it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. A network is typically called a deep neural network if it has at least 2 hidden layers.[102]\n",
      "Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. Text-based GPT models are pre-trained on a large corpus of text which can be from the internet. The application of AI in medicine and medical research has the potential to increase patient care and quality of life.[126] Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients. For medical research, AI is an important tool for processing and integrating Big Data. champions, Brad Rutter and Ken Jennings, by a significant margin. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management. This is especially true when using AI algorithms that are inherently unexplainable in deep learning.[134]\n",
      "Machine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright. In 2022, generative AI began to create images, audio, video and text that are indistinguishable from real photographs, recordings, films or human writing. Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.[154]\n",
      "COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. This system uses cameras to monitor traffic density and adjust signal timing based on the interval needed to clear traffic.[181] Terrorists, criminals and rogue states can use weaponized AI such as advanced digital warfare and lethal autonomous weapons. Things like ideologies, law, government, money and the economy are made of language; they exist because there are stories that billions of people believe. \"[204] While the tools that are now being used to improve lives can also be used by bad actors, \"they can also be used against the bad actors. Then followed three radio broadcasts on AI by Turing, the lectures: 'Intelligent Machinery, A Heretical Theory’, ‘Can Digital Computers Think’? By 1985, the market for AI had reached over a billion dollars. They were highly successful at \"intelligent\" tasks such as algebra or IQ tests. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. David Chalmers identified two problems in understanding the mind, which he named the \"hard\" and \"easy\" problems of consciousness.[284] The easy problem is understanding how the brain processes signals, makes plans and controls behavior.\n",
      "\n",
      "Summary from Google link:\n",
      " We use cookies to ensure that we give you the best experience on our website.• Privacy policy• Cookie policy A startup ecosystem enabler aimed at supporting startups by offering them long standing experience at all stages. We empower the startup ecosystem by providing networking, mentorship, advisory, and training for our members. The ability to create a scalable, sustainable, and investor-friendly startup in the ecosystem depends on their ability to acquire and use the appropriate entrepreneurship, technological training and capacity development.\n",
      "\n",
      "Summary from PDF:\n",
      " MARKETING\n",
      "PROPOSAL\n",
      "Content strategies and content calendar \n",
      "Bush MwauraPRESENTED TO:\n",
      "SheramugoPRESENTED BY:          VISION:\n",
      "Your vison is to scale this year . Get a good following and generate lads and also\n",
      "enhance your trust with the existing customers. Engagement: Actively respond to comments, direct messages, and mentions. User-Generated Content: Encourage customers to share their projects using your\n",
      "services. - Create mood boards for different industries to demonstrate design possibilities. - Offer design tutorials or tips to empower your audience- Both of these two as explained before\n",
      "places you at an authority level in your field or print industry plus it helps when people go through your\n",
      "page , the do some saves and shares that help boost your content \n",
      "5. Holiday and Seasonal Campaigns:\n",
      "   - Design special promotions or discounts for holidays and seasons. - Reflect on the company's growth journey, showcasing progress over time. Print Duka's Sustainability Initiatives:\n",
      "    - Highlight eco-friendly printing options and sustainable practices.\n"
     ]
    }
   ],
   "source": [
    "summary_from_url = generate_summary(url_text)\n",
    "summary_from_google = generate_summary(google_text)\n",
    "summary_from_pdf = generate_summary(pdf_text)\n",
    "\n",
    "print(\"Summary from URL:\\n\", summary_from_url)\n",
    "print(\"\\nSummary from Google link:\\n\", summary_from_google)\n",
    "print(\"\\nSummary from PDF:\\n\", summary_from_pdf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
